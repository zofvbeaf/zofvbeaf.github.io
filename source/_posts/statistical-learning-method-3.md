---
title: 《统计学习方法》三. K近邻法
tags:
  - 统计学习方法
  - machine learning
  - book
categories:
  - 统计学习方法
date: 2018-11-09 16:43:15
mathjax: true
---

$k$-NN ($k$-nearest neighbor)
==============
> 一种基本的分类与回归的方法


## 算法
+ 实例 $x$ 所属的类$y$, 有: 
$$\displaystyle y = \arg\max_{c_j}\sum_{x_i \in N_k(x)} I(y_i = c_j), \;\;\;\; i = 1,2,\cdots ,N; \;\; j=1,2,\cdots ,K$$
+ $k=1$时为最近邻法

## $k$近邻模型
+ 模型三要素: 距离度量, $k$值的选择和分类决策规则的确定
+ 距离度量
  + 一般用欧式距离, 也可以是其它距离, 如$L_p$距离(Minkowski距离): $\displaystyle L_p(x_i, x_j) = \left(\sum_{l = 1}^n |x_i^{(l)} - x_j^{(l)}|^p\right)^{\frac{1}{p}}$
  > 欧式距离: $p=2$
  > 曼哈顿距离: $p=1$
  > 各个坐标距离的最大值: $p=\infty$
+ $k$的选择
  + 较小: 近似误差小, 估计误差大
  + 较大: 近似误差大, 估计误差小
  + 应用中通常选取较小的$k$值, 采用交叉验证法选取最优的$k$值
+ 分类决策规则
  + 经验风险最小化: 即$\displaystyle \sum_{x_i \in N_k(x)} I(y_i = c_j)$最大化

## $k$近邻法的实现: $kd$树
+ $kd$树是一种对$k$维空间进行存储以便对其进行快速检索的树形数据结构.
+ $kd$树的每个节点对应于一个$k$维超矩形区域.
+ 此处的$k$与$k$近邻法的$k$不同.
+ $kd$树搜索的平均时间复杂度为$O(\log N)$, 更适用于训练实例数远大于空间维数时的$k$近邻搜索.
+ 当空间维数接近训练实例数时, 它的效率会迅速下降, 几乎接近线性扫描.

