---
title: 《统计学习方法》五.决策树
tags:
  - 统计学习方法
  - machine learning
  - book
categories:
  - 统计学习方法
date: 2018-11-10 20:03:11
mathjax: true
---


决策树
================
+ 决策树是一种基本的分类与回归方法
+ 本质上是从训练数据集归纳出一组分类规则
+ 决策树学习通常包括三个步骤: 特征选择, 决策树的生成和决策树的修剪
+ 内部节点表示一个特征或属性, 叶节点表示一个类

决策树模型与学习
--------------------
+ 损失函数通常是正则化的极大似然函数
+ 需要自下而上进行剪枝, 去掉过于细分的叶节点, 使其回退到父节点, 甚至更高的节点, 避免过拟合, 使其有更好的泛化能力
+ 决策树的生成只考虑局部最优, 决策树的生成则考虑全局最优

特征选择
--------------------
+ 通常的特征选择的准则是信息增益或信息增益比
+ 熵(**entropy**): $\displaystyle H(p) = \sum_{i=1}^n p_i \log p_i$
+ 条件熵(**conditional entropy**): $\displaystyle H(Y|X) = \sum_{i=1}^n p_i H(Y|X=x_i)$
+ 当熵和条件熵中的概率由数据估计(特别是极大似然估计得到)时, 所对应的熵与条件熵分别称为经验熵与经验条件熵
+ 信息增益表示得知特征$X$的信息而使得类$Y$的信息的不确定性减少的程度
+ 特征$A$对训练数据集$D$的信息增益$g(D,A) = H(D) - H(D|A)$, 也成为互信息(**mutual information**)
+ 信息增益比: 
  $$g_R (D,A) = \frac{g(D,A)}{H_A(D)}$$ 
  其中，$H_A (D)$表示训练数据集$D$关于特征$A$的值的熵
  $$\displaystyle H_A (D) = -\sum_{i=1}^n \frac{|D_i|}{D} \log_2 \frac{|D_i|}{D}$$

决策树的生成
--------------------

### ID3算法
> 输入: 训练数据集D, 特征集A, 阈值\varepsilon
> 每次选择$ g(D,A) $最大的特征点递归构建, 直到所有特征的$g(D,A)$均很小($\lt\varepsilon$)或没有特征可以选择为止

### C4.5算法
+ 用信息增益比来选择特征


决策树的剪枝
--------------------
+ 损失函数: $C_\alpha (T) = C(T) + \alpha |T|$, 其中 
$$\displaystyle C(T) = \sum_{t=1}^{|T|}N_t H_t (T) \;\;\;\;\;  H_t (T) = -\sum_k \frac{N_{tk}}{N_t} \log \frac{N_{tk}}{N_t}$$
$N_{tk}$表示树$T$的某一叶节点$t$的第$k$类样本点的数量
+ 若一组叶节点回缩前后的树分别为$T_B$和$T_A$, 当$C_\alpha (T_A) \le C_\alpha (T_B)$时进行剪枝, 将父节点变为新的叶节点
+ 利用损失函数最小原则进行剪枝就是用正则化的极大似然估计进行模型选择 


CART算法
-------------------
> 分类与回归树(Classification and Regression Trees)
> 递归构建二叉决策树再剪枝
> 具体见《统计学习方法》

### CART生成
+ 回归树的生成: 用平方误差最小化准则, 最小二乘回归树生成算法
+ 分类树的生成: 用基尼系数选择最优特征, 同时决定该特征的最优二值切分点

### CART剪枝
+ 首先从生成算法产生的决策树$T_0$底端开始不断剪枝, 直到$T_0$的根节点, 形成一个子树序列$\lbrace T_0, T_1, \cdots, T_n \rbrace$; 
然后通过交叉验证法再独立的验证数据集上对子树序列进行测试, 从中选择最优子树






